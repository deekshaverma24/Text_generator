{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "Wdx_CkXKYl4N",
    "outputId": "8ca1f654-86a2-4461-b516-7871c5f332c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import dependencies\n",
    "import numpy\n",
    "import sys\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JlXUtla7Yy0r"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "#loading the data and opening our input data in the form of a text file\n",
    "#Project Gutenburg/berg is where the data can be found (Just Google it!)\n",
    "file = open(\"frankenstein-2.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zAZ-z3hgYzNp"
   },
   "outputs": [],
   "source": [
    "# tokenization\n",
    "#standardisation\n",
    "#What is tokenization? Tokenization is the process of breaking a stream of text up into words phrases, symbols or other meaningful elements\n",
    "def tokenize_words(input):\n",
    "    #lowercase everything to standardise it\n",
    "    input = input.lower()\n",
    "    #initiating the tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # tokenizing the text into tokens\n",
    "    tokens = tokenizer.tokenize(input)\n",
    "    # filtering the stopwords using lambda\n",
    "    filtered = filter(lambda token: token not in stopwords.words('english'), tokens)\n",
    "    return \" \".join(filtered)\n",
    "\n",
    "#process the input data and make tokens\n",
    "processed_inputs = tokenize_words(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4OssYUSeYztp"
   },
   "outputs": [],
   "source": [
    "#chars to numbers\n",
    "# convert characters in our input to numbers\n",
    "# we'll sort the list of the sets of all characters that appear in out i/p text and then use the enumerate fn to get the numbers that represents the characters\n",
    "#we'll then create a dictionary that stores the keys and values, or the characters and the numbers that represents them\n",
    "chars = sorted(list(set(processed_inputs)))\n",
    "char_to_num = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "6Fwj5yixYz_6",
    "outputId": "5d245f6b-b48f-4160-f4a7-8c347e6aa1e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 269913\n",
      "Total vocab: 42\n"
     ]
    }
   ],
   "source": [
    "# check if words to chars or chars to num (?!) has worked?\n",
    "# just so we get an idea of whether our process of converting words to characters has worked, we print the length of our variables\n",
    "input_len = len(processed_inputs)\n",
    "vocab_len = len(chars)\n",
    "print (\"Total number of characters:\", input_len)\n",
    "print (\"Total vocab:\", vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aIIXN0lQY0Qm"
   },
   "outputs": [],
   "source": [
    "# seq length\n",
    "#we're defining how long we want an individual sequence here\n",
    "#an individual squence is a complete mapping of input characters as integers\n",
    "seq_length = 100\n",
    "x_data = []\n",
    "y_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "6Vy7sBrkY0jv",
    "outputId": "25b9b861-8a49-46cf-ad4d-e479d1f004e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns: 269813\n"
     ]
    }
   ],
   "source": [
    "# loop through the sequence\n",
    "#here we're going through the entire list of i/ps and converting the chars to numbers with a for loop\n",
    "# this will create a bunch of sequences where each sequence starts with the next characters in the i/p data beginning with the first character \n",
    "for i in range(0, input_len - seq_length, 1):\n",
    "    #define i/p and o/p sequences\n",
    "    #i/p is the current character plus the desired sequence length\n",
    "    in_seq = processed_inputs[i:i + seq_length]\n",
    "    # out sequence is the initial character plus total sequence length\n",
    "\n",
    "    out_seq = processed_inputs[i + seq_length]\n",
    "    # converting the list of charaters to integers based on previous values and appending the values to our lists\n",
    "    x_data.append([char_to_num[char] for char in in_seq])\n",
    "    y_data.append(char_to_num[out_seq])\n",
    "\n",
    "#check to see how many total input sequences we have\n",
    "n_patterns = len(x_data)\n",
    "print (\"Total Patterns:\", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lc9wbvCaY0x2"
   },
   "outputs": [],
   "source": [
    "# convert input sequence to np array that our network can use\n",
    "X = numpy.reshape(x_data, (n_patterns, seq_length, 1))\n",
    "X = X/float(vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D5Zw87JkY1AP"
   },
   "outputs": [],
   "source": [
    "#one-hot coding our label data\n",
    "y = np_utils.to_categorical(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yrfguvt1Y1NF"
   },
   "outputs": [],
   "source": [
    "#creating the model\n",
    "# creating a sequential model\n",
    "# dropout is used to prevent overfitting\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o4Ff6eqkY1a2"
   },
   "outputs": [],
   "source": [
    "#compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x_TN7FCmY1m_"
   },
   "outputs": [],
   "source": [
    "# saving weights\n",
    "filepath = \"model_weights_saved.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "desired_callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "8Lcdb7XSY1zL",
    "outputId": "5feb1c8e-4ca5-4911-90f8-3a82565d0f0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "269813/269813 [==============================] - 4779s 18ms/step - loss: 2.9057\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.90572, saving model to model_weights_saved.hdf5\n",
      "Epoch 2/4\n",
      "269813/269813 [==============================] - 4556s 17ms/step - loss: 2.6106\n",
      "\n",
      "Epoch 00002: loss improved from 2.90572 to 2.61057, saving model to model_weights_saved.hdf5\n",
      "Epoch 3/4\n",
      "269813/269813 [==============================] - 4472s 17ms/step - loss: 2.4314\n",
      "\n",
      "Epoch 00003: loss improved from 2.61057 to 2.43139, saving model to model_weights_saved.hdf5\n",
      "Epoch 4/4\n",
      "269813/269813 [==============================] - 4531s 17ms/step - loss: 2.3020\n",
      "\n",
      "Epoch 00004: loss improved from 2.43139 to 2.30203, saving model to model_weights_saved.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f3e5b314668>"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model and let it train\n",
    "model.fit(X, y, epochs=4, batch_size=256, callbacks=desired_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0w-2W4U0Y1_M"
   },
   "outputs": [],
   "source": [
    "#recompile model with the saved weights\n",
    "filename = \"model_weights_saved.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jcppno4KY2Lv"
   },
   "outputs": [],
   "source": [
    "#output of the model back into characters\n",
    "num_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "rUv70V75Y2ZQ",
    "outputId": "f89d3bc2-36e9-4233-b213-7323f00f1b9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:\n",
      "\" ild lost alarmed account passed several hours looking gates geneva shut forced remain several hours  \"\n"
     ]
    }
   ],
   "source": [
    "# random seed to help generate\n",
    "start = numpy.random.randint(0, len(x_data) - 1)\n",
    "pattern = x_data[start]\n",
    "print(\"Random Seed:\")\n",
    "print(\"\\\"\",''.join([num_to_char[value] for value in pattern]),\"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "ZHVaPETqY2mz",
    "outputId": "073d5d82-57d2-4c1b-8774-2b54a570d9ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sear"
     ]
    }
   ],
   "source": [
    "# generate the text\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern,(1,len(pattern),1))\n",
    "    x = x/float(vocab_len)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = num_to_char[index]\n",
    "    seq_in = [num_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g7S8_uVaY20F"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Text Generator Project1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
